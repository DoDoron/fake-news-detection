# Custom Dataset ì‚¬ìš© ê°€ì´ë“œ

Dataset 1, 3, 4ë¥¼ í•©ì³ì„œ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ì„ ë§Œë“¤ê³  í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ê°œìš”

ì´ ì‹œìŠ¤í…œì€ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤:

### Train ë°ì´í„° êµ¬ì„±
- **Dataset 1**: train.csv + val.csv (ì „ì²´ ì‚¬ìš©)
- **Dataset 3**: train.csv (ìƒ˜í”Œë§ ë¹„ìœ¨ ì¡°ì • ê°€ëŠ¥)
- **Dataset 4**: train.csv (ìƒ˜í”Œë§ ë¹„ìœ¨ ì¡°ì • ê°€ëŠ¥)

### Validation ë°ì´í„° êµ¬ì„±
- ìœ„ì˜ merged train ë°ì´í„°ë¥¼ train/valë¡œ split (ë¹„ìœ¨ ì¡°ì • ê°€ëŠ¥)

### Test ë°ì´í„° êµ¬ì„±
- **Dataset 1**: test.csv (ì „ì²´ ì‚¬ìš©)
- **Dataset 3**: test.csv (ì „ì²´ ì‚¬ìš©)
- **Dataset 4**: test.csv (ì „ì²´ ì‚¬ìš©)

## ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„°

ë‹¤ìŒ 3ê°€ì§€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **`--dataset3-ratio`**: Dataset 3ì˜ ìƒ˜í”Œë§ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1 = 10%)
2. **`--dataset4-ratio`**: Dataset 4ì˜ ìƒ˜í”Œë§ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.01 = 1%)
3. **`--train-val-ratio`**: Train/Val split ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.8 = 80% train, 20% val)

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ë°©ë²• 1: ëª…ë ¹ì¤„ì—ì„œ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
python prepare_custom_dataset.py

# ìƒ˜í”Œë§ ë¹„ìœ¨ ë³€ê²½
python prepare_custom_dataset.py --dataset3-ratio 0.2 --dataset4-ratio 0.05

# Train/Val ë¹„ìœ¨ ë³€ê²½ (90:10)
python prepare_custom_dataset.py --train-val-ratio 0.9

# ëª¨ë“  íŒŒë¼ë¯¸í„° ì§€ì •
python prepare_custom_dataset.py \
    --dataset3-ratio 0.15 \
    --dataset4-ratio 0.02 \
    --train-val-ratio 0.85 \
    --seed 123 \
    --output-name my-custom-dataset
```

### ë°©ë²• 2: ë…¸íŠ¸ë¶ì—ì„œ ì‹¤í–‰

`notebooks/train_custom_dataset.ipynb` íŒŒì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.

```python
# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
DATASET_3_RATIO = 0.1
DATASET_4_RATIO = 0.01
TRAIN_VAL_RATIO = 0.8
RANDOM_SEED = 42
OUTPUT_DATASET_NAME = "custom-dataset"

# ë°ì´í„°ì…‹ ì¤€ë¹„ (ì…¸ ëª…ë ¹ì–´ë¡œ ì‹¤í–‰)
!cd .. && python prepare_custom_dataset.py \
    --dataset3-ratio {DATASET_3_RATIO} \
    --dataset4-ratio {DATASET_4_RATIO} \
    --train-val-ratio {TRAIN_VAL_RATIO} \
    --seed {RANDOM_SEED} \
    --output-name {OUTPUT_DATASET_NAME}

# ë°ì´í„° ë¡œë“œ ë° í•™ìŠµ
loaders, vocab, tokenizer, info = build_datasets(
    name=OUTPUT_DATASET_NAME,
    batch_size=64,
    max_len=512,
    num_workers=4,
    max_vocab_size=20000,
)
```

## ğŸ“Š ì‹¤í—˜ ì˜ˆì‹œ

### ì‹¤í—˜ 1: ìƒ˜í”Œë§ ë¹„ìœ¨ ë³€í™”

```bash
# ì ì€ ìƒ˜í”Œë§
python prepare_custom_dataset.py \
    --dataset3-ratio 0.05 \
    --dataset4-ratio 0.005 \
    --output-name custom-small

# ì¤‘ê°„ ìƒ˜í”Œë§ (ê¸°ë³¸ê°’)
python prepare_custom_dataset.py \
    --dataset3-ratio 0.1 \
    --dataset4-ratio 0.01 \
    --output-name custom-medium

# ë§ì€ ìƒ˜í”Œë§
python prepare_custom_dataset.py \
    --dataset3-ratio 0.2 \
    --dataset4-ratio 0.02 \
    --output-name custom-large
```

### ì‹¤í—˜ 2: Train/Val ë¹„ìœ¨ ë³€í™”

```bash
# 70:30 split
python prepare_custom_dataset.py --train-val-ratio 0.7 --output-name custom-70-30

# 80:20 split (ê¸°ë³¸ê°’)
python prepare_custom_dataset.py --train-val-ratio 0.8 --output-name custom-80-20

# 90:10 split
python prepare_custom_dataset.py --train-val-ratio 0.9 --output-name custom-90-10
```

## ğŸ“ ì¶œë ¥ êµ¬ì¡°

ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ë°ì´í„°ì…‹ì´ ìƒì„±ë©ë‹ˆë‹¤:

```
dataset/
  custom-dataset/          # ë˜ëŠ” ì§€ì •í•œ output-name
    â”œâ”€â”€ train.csv          # Train ë°ì´í„°
    â”œâ”€â”€ val.csv            # Validation ë°ì´í„°
    â””â”€â”€ test.csv           # Test ë°ì´í„°
```

ê° CSV íŒŒì¼ì€ ë‹¤ìŒ ì»¬ëŸ¼ì„ í¬í•¨í•©ë‹ˆë‹¤:
- `title`: ê¸°ì‚¬ ì œëª©
- `text`: ê¸°ì‚¬ ë³¸ë¬¸
- `label`: ë ˆì´ë¸” (0: real, 1: fake)

## ğŸ” ë°ì´í„° í†µê³„ í™•ì¸

ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ ë‹¤ìŒ ì •ë³´ê°€ ì¶œë ¥ë©ë‹ˆë‹¤:

```
=== Preparing Train/Val Data ===
Dataset 3 sample ratio: 10.0%
Dataset 4 sample ratio: 1.0%
Train/Val split ratio: 0.80/0.20

Loading dataset 1 - train: train.csv
  - Loaded 26938 rows
Loading dataset 1 - val: val.csv
  - Loaded 8979 rows
Dataset 1 total: 35917 rows

Loading dataset 3 - train: train.csv
  - Sampled 2694/26938 rows (10.0%)

Loading dataset 4 - train: train.csv
  - Sampled 2320/232003 rows (1.0%)

Combined total: 40931 rows
Label distribution:
  - 0: 20465 (50.0%)
  - 1: 20466 (50.0%)

Split results:
  - Train: 32745 rows
  - Val: 8186 rows
```

## ğŸ“ í•™ìŠµ ì‹¤í–‰

ë°ì´í„°ì…‹ì´ ìƒì„±ë˜ë©´ ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from core.data import build_datasets
from core.train_eval import train_and_evaluate

# ë°ì´í„° ë¡œë“œ
loaders, vocab, tokenizer, info = build_datasets(
    name='custom-dataset',
    batch_size=64,
    max_len=512,
    num_workers=4,
    max_vocab_size=20000,
)

# ëª¨ë¸ í•™ìŠµ
model = MODEL_REGISTRY['bilstm'](vocab_size=len(vocab), num_classes=2)
results, run_dir = train_and_evaluate(
    model, loaders, config,
    dataset_name='custom-dataset',
    model_name='bilstm',
)
```

## ğŸ’¡ íŒ

1. **ì²˜ìŒ ì‹¤í—˜**: ì‘ì€ ìƒ˜í”Œë§ ë¹„ìœ¨ë¡œ ì‹œì‘í•´ì„œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
   ```bash
   python prepare_custom_dataset.py --dataset3-ratio 0.01 --dataset4-ratio 0.001
   ```

2. **ì¬í˜„ì„±**: ë™ì¼í•œ ê²°ê³¼ë¥¼ ìœ„í•´ `--seed` ê°’ì„ ê³ ì •
   ```bash
   python prepare_custom_dataset.py --seed 42
   ```

3. **í´ë˜ìŠ¤ ë¶ˆê· í˜•**: ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ stratified splitì„ ìˆ˜í–‰í•˜ì—¬ train/valì˜ ë ˆì´ë¸” ë¹„ìœ¨ì„ ë™ì¼í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤.

4. **ë©”ëª¨ë¦¬ ë¶€ì¡±**: Dataset 3, 4ì˜ ìƒ˜í”Œë§ ë¹„ìœ¨ì„ ë‚®ì¶”ì„¸ìš”.

## ğŸ› ë¬¸ì œ í•´ê²°

### ì—ëŸ¬: "Dataset file not found"
- ë°ì´í„°ì…‹ íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ê²½ë¡œì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
- ê²½ë¡œ: `dataset/fake-news-classification/`, `dataset/fake-news-detection-datasets/`, `dataset/llm-fake-news/`

### ì—ëŸ¬: "must contain 'label' column"
- CSV íŒŒì¼ì— `label` ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”

### ë©”ëª¨ë¦¬ ë¶€ì¡±
- ìƒ˜í”Œë§ ë¹„ìœ¨ì„ ë‚®ì¶”ì„¸ìš”
- ë˜ëŠ” batch_sizeë¥¼ ì¤„ì´ì„¸ìš”

## ğŸ“ ì°¸ê³ ì‚¬í•­

- Train/Val split ì‹œ **stratified sampling**ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì´ë¸” ë¹„ìœ¨ì„ ìœ ì§€í•©ë‹ˆë‹¤
- Test ë°ì´í„°ëŠ” ìƒ˜í”Œë§í•˜ì§€ ì•Šê³  ì „ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
- ëª¨ë“  ë°ì´í„°ëŠ” `title`, `text`, `label` ì»¬ëŸ¼ìœ¼ë¡œ ì •ê·œí™”ë©ë‹ˆë‹¤
- ê²°ì¸¡ì¹˜ëŠ” ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤ (title/textëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ, label ì—†ëŠ” í–‰ì€ ì œê±°)

