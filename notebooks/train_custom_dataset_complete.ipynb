{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset Training\n",
    "Dataset 1, 3, 4Î•º Ìï©Ï≥êÏÑú ÌïôÏäµÌïòÎäî ÎÖ∏Ìä∏Î∂Å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d88c3",
   "metadata": {},
   "source": [
    "## 0. GPU ÌôïÏù∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üñ•Ô∏è  GPU ÌôïÏù∏\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA ÏÇ¨Ïö© Í∞ÄÎä•!\")\n",
    "    print(f\"   GPU Ïù¥Î¶Ñ: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Í∞úÏàò: {torch.cuda.device_count()}\")\n",
    "    print(f\"   ÌòÑÏû¨ GPU: cuda:{torch.cuda.current_device()}\")\n",
    "    print(f\"   Î©îÎ™®Î¶¨: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"\\n‚ú® ÌïôÏäµ Ïãú GPUÍ∞Ä ÏûêÎèôÏúºÎ°ú ÏÇ¨Ïö©Îê©ÎãàÎã§!\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA ÏÇ¨Ïö© Î∂àÍ∞Ä (CPUÎ°ú ÌïôÏäµÎê©ÎãàÎã§)\")\n",
    "    print(\"   GPUÎ•º ÏÇ¨Ïö©ÌïòÎ†§Î©¥:\")\n",
    "    print(\"   1. NVIDIA GPUÍ∞Ä ÏÑ§ÏπòÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏\")\n",
    "    print(\"   2. CUDA Toolkit ÏÑ§Ïπò\")\n",
    "    print(\"   3. PyTorch CUDA Î≤ÑÏ†Ñ ÏÑ§Ïπò\")\n",
    "    print(\"      pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import display, Image\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from core.config import load_training_config\n",
    "from core.data import available_datasets, build_datasets\n",
    "from core.train_eval import train_and_evaluate\n",
    "from core.utils import setup_logging\n",
    "from model import MODEL_REGISTRY\n",
    "\n",
    "setup_logging()\n",
    "CONFIG_PATH = PROJECT_ROOT / 'configs' / 'default.yaml'\n",
    "BASE_CONFIG = load_training_config(CONFIG_PATH)\n",
    "RUN_HISTORY = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï\n",
      "============================================================\n",
      "Dataset 3 sampling : 10.0% -> 2694 rows\n",
      "Dataset 4 sampling : 1.0% -> 2320 rows\n",
      "Train/Val ratio: 80% / 20%\n",
      "Random seed: 42\n",
      "Output dataset name: custom-dataset\n",
      "Model: bilstm\n",
      "Epochs: 5\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉòÌîåÎßÅ ÎπÑÏú®\n",
    "DATASET_3_RATIO = 0.1   # Dataset 3 ÏÉòÌîåÎßÅ ÎπÑÏú® (10%)\n",
    "DATASET_4_RATIO = 0.01  # Dataset 4 ÏÉòÌîåÎßÅ ÎπÑÏú® (1%)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉòÌîåÎßÅ ÎπÑÏú®Ïóê Îî∞Î•∏ Í∞ØÏàò\n",
    "DATASET_3_LEN = 26938\n",
    "DATASET_4_LEN = 232003\n",
    "DATASET_3_SAMPLE_LEN = 26938 * DATASET_3_RATIO\n",
    "DATASET_4_SAMPLE_LEN = 232003 * DATASET_4_RATIO\n",
    "\n",
    "# Train/Val split ÎπÑÏú®\n",
    "TRAIN_VAL_RATIO = 0.8   # Train 80%, Val 20%\n",
    "\n",
    "# ÎûúÎç§ ÏãúÎìú\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Ï∂úÎ†• Îç∞Ïù¥ÌÑ∞ÏÖã Ïù¥Î¶Ñ\n",
    "OUTPUT_DATASET_NAME = \"custom-dataset\"\n",
    "\n",
    "# Î™®Îç∏ Î∞è ÌïôÏäµ ÏÑ§Ï†ï\n",
    "MODEL_NAME = 'bilstm'  # 'bow_mlp', 'cnn_text', 'bilstm', 'tiny_transformer'\n",
    "EPOCHS = 5\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset 3 sampling : {DATASET_3_RATIO:.1%} -> {round(DATASET_3_SAMPLE_LEN)} rows\")\n",
    "print(f\"Dataset 4 sampling : {DATASET_4_RATIO:.1%} -> {round(DATASET_4_SAMPLE_LEN)} rows\")\n",
    "print(f\"Train/Val ratio: {TRAIN_VAL_RATIO:.0%} / {(1-TRAIN_VAL_RATIO):.0%}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "print(f\"Output dataset name: {OUTPUT_DATASET_NAME}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑ\n",
    "\n",
    "**Ï£ºÏùò**: Îç∞Ïù¥ÌÑ∞ÏÖãÏù¥ Ïù¥ÎØ∏ ÏÉùÏÑ±ÎêòÏñ¥ ÏûàÏúºÎ©¥ Ïù¥ ÏÖÄÏùÑ Í±¥ÎÑàÎõ∞ÏÑ∏Ïöî."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-28 17:43:22,836 - INFO - \n",
      "=== Preparing Train/Val Data ===\n",
      "2025-10-28 17:43:22,836 - INFO - Dataset 3 sample ratio: 10.0%\n",
      "2025-10-28 17:43:22,836 - INFO - Dataset 4 sample ratio: 1.0%\n",
      "2025-10-28 17:43:22,836 - INFO - Train/Val split ratio: 0.80/0.20\n",
      "2025-10-28 17:43:22,836 - INFO - Loading dataset 1 - train: train.csv\n",
      "2025-10-28 17:43:23,273 - INFO -   - Loaded 24353 rows\n",
      "2025-10-28 17:43:23,273 - INFO - Loading dataset 1 - val: val.csv\n",
      "2025-10-28 17:43:23,404 - INFO -   - Loaded 8117 rows\n",
      "2025-10-28 17:43:23,405 - INFO - Dataset 1 total: 32470 rows\n",
      "2025-10-28 17:43:23,405 - INFO - Loading dataset 3 - train: train.csv\n",
      "2025-10-28 17:43:23,822 - INFO -   - Sampled 2694/26938 rows (10.0%)\n",
      "2025-10-28 17:43:23,822 - INFO - Loading dataset 4 - train: train.csv\n",
      "2025-10-28 17:43:24,838 - WARNING - train.csv does not have 'title' column. Using empty string.\n",
      "2025-10-28 17:43:24,975 - INFO -   - Sampled 2320/232003 rows (1.0%)\n",
      "2025-10-28 17:43:24,977 - INFO - \n",
      "Combined total: 37484 rows\n",
      "2025-10-28 17:43:24,978 - INFO - Label distribution:\n",
      "2025-10-28 17:43:24,978 - INFO -   - 1: 19718 (52.6%)\n",
      "2025-10-28 17:43:24,978 - INFO -   - 0: 17766 (47.4%)\n",
      "2025-10-28 17:43:24,985 - INFO - \n",
      "Split results:\n",
      "2025-10-28 17:43:24,985 - INFO -   - Train: 29986 rows\n",
      "2025-10-28 17:43:24,985 - INFO -   - Val: 7498 rows\n",
      "2025-10-28 17:43:24,985 - INFO - \n",
      "Train label distribution:\n",
      "2025-10-28 17:43:24,985 - INFO -   - 1: 15774 (52.6%)\n",
      "2025-10-28 17:43:24,985 - INFO -   - 0: 14212 (47.4%)\n",
      "2025-10-28 17:43:24,985 - INFO - \n",
      "Val label distribution:\n",
      "2025-10-28 17:43:24,986 - INFO -   - 1: 3944 (52.6%)\n",
      "2025-10-28 17:43:24,986 - INFO -   - 0: 3554 (47.4%)\n",
      "2025-10-28 17:43:24,986 - INFO - \n",
      "=== Preparing Test Data ===\n",
      "2025-10-28 17:43:24,986 - INFO - Loading dataset 1 - test: test.csv\n",
      "2025-10-28 17:43:25,154 - INFO -   - Loaded 8117 rows\n",
      "2025-10-28 17:43:25,154 - INFO - Loading dataset 3 - test: test.csv\n",
      "2025-10-28 17:43:25,316 - INFO -   - Loaded 8981 rows\n",
      "2025-10-28 17:43:25,316 - INFO - Loading dataset 4 - test: test.csv\n",
      "2025-10-28 17:43:25,403 - WARNING - test.csv does not have 'title' column. Using empty string.\n",
      "2025-10-28 17:43:25,419 - INFO -   - Loaded 38666 rows\n",
      "2025-10-28 17:43:25,420 - INFO - \n",
      "Test total: 55764 rows\n",
      "2025-10-28 17:43:25,420 - INFO - Test label distribution:\n",
      "2025-10-28 17:43:25,420 - INFO -   - 0: 35495 (63.7%)\n",
      "2025-10-28 17:43:25,420 - INFO -   - 1: 20269 (36.3%)\n",
      "2025-10-28 17:43:26,618 - INFO - \n",
      "=== Datasets Saved ===\n",
      "2025-10-28 17:43:26,618 - INFO - Output directory: /Users/dorong/Desktop/hackerthonTA/fake-news-detection/dataset/custom-dataset\n",
      "2025-10-28 17:43:26,618 - INFO -   - Train: train.csv (29986 rows)\n",
      "2025-10-28 17:43:26,618 - INFO -   - Val: val.csv (7498 rows)\n",
      "2025-10-28 17:43:26,618 - INFO -   - Test: test.csv (55764 rows)\n",
      "2025-10-28 17:43:26,619 - INFO - \n",
      "‚úì Dataset preparation completed successfully!\n",
      "2025-10-28 17:43:26,619 - INFO - \n",
      "To use this dataset, run:\n",
      "2025-10-28 17:43:26,619 - INFO -   DATASET_NAME = 'custom-dataset'\n"
     ]
    }
   ],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑ Ïä§ÌÅ¨Î¶ΩÌä∏ Ïã§Ìñâ\n",
    "!cd .. && python prepare_custom_dataset.py \\\n",
    "    --dataset3-ratio {DATASET_3_RATIO} \\\n",
    "    --dataset4-ratio {DATASET_4_RATIO} \\\n",
    "    --train-val-ratio {TRAIN_VAL_RATIO} \\\n",
    "    --seed {RANDOM_SEED} \\\n",
    "    --output-name {OUTPUT_DATASET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Îç∞Ïù¥ÌÑ∞ÏÖã ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset found: /Users/dorong/Desktop/hackerthonTA/fake-news-detection/dataset/custom-dataset\n",
      "\n",
      "TRAIN:\n",
      "  - Total: 29986 rows\n",
      "  - Label 1: 15774 (52.6%)\n",
      "  - Label 0: 14212 (47.4%)\n",
      "\n",
      "VAL:\n",
      "  - Total: 7498 rows\n",
      "  - Label 1: 3944 (52.6%)\n",
      "  - Label 0: 3554 (47.4%)\n",
      "\n",
      "TEST:\n",
      "  - Total: 55764 rows\n",
      "  - Label 0: 35495 (63.7%)\n",
      "  - Label 1: 20269 (36.3%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ÏÉùÏÑ±Îêú Îç∞Ïù¥ÌÑ∞ÏÖã ÌôïÏù∏\n",
    "dataset_path = PROJECT_ROOT / 'dataset' / OUTPUT_DATASET_NAME\n",
    "\n",
    "if dataset_path.exists():\n",
    "    print(f\"‚úÖ Dataset found: {dataset_path}\\n\")\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        file_path = dataset_path / f\"{split}.csv\"\n",
    "        if file_path.exists():\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"{split.upper()}:\")\n",
    "            print(f\"  - Total: {len(df)} rows\")\n",
    "            label_dist = df['label'].value_counts()\n",
    "            for label, count in label_dist.items():\n",
    "                print(f\"  - Label {label}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "            print()\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found: {dataset_path}\")\n",
    "    print(\"Please run the data preparation cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Î™®Îç∏ ÌïôÏäµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: custom-dataset\n",
      "Model: bilstm\n",
      "Epochs: 5\n",
      "\n",
      "\n",
      "Vocab size: 19996\n",
      "Batch size: 64\n",
      "Max length: 256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-28 17:45:47] INFO fake_news: Starting training | dataset=custom-dataset model=bilstm epochs=5 batch_size=64\n",
      "[2025-10-28 17:56:54] INFO fake_news: Epoch 1 | train_loss=0.3743 val_loss=0.3318 val_f1=0.8850\n",
      "[2025-10-28 18:11:19] INFO fake_news: Epoch 2 | train_loss=0.3131 val_loss=0.3268 val_f1=0.8869\n",
      "[2025-10-28 18:23:45] INFO fake_news: Epoch 3 | train_loss=0.3013 val_loss=0.3202 val_f1=0.8944\n",
      "[2025-10-28 18:37:16] INFO fake_news: Epoch 4 | train_loss=0.2869 val_loss=0.3212 val_f1=0.8992\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = OUTPUT_DATASET_NAME\n",
    "\n",
    "print(f\"Loading dataset: {DATASET_NAME}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Epochs: {EPOCHS}\\n\")\n",
    "\n",
    "config = load_training_config(CONFIG_PATH, overrides={'epochs': EPOCHS})\n",
    "loaders, vocab, tokenizer, info = build_datasets(\n",
    "    name=DATASET_NAME,\n",
    "    batch_size=config.batch_size,\n",
    "    max_len=config.max_len,\n",
    "    num_workers=config.num_workers,\n",
    "    max_vocab_size=20000,\n",
    ")\n",
    "\n",
    "print(f\"\\nVocab size: {len(vocab)}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Max length: {config.max_len}\\n\")\n",
    "\n",
    "model_cls = MODEL_REGISTRY[MODEL_NAME]\n",
    "model = model_cls(vocab_size=len(vocab), num_classes=2)\n",
    "\n",
    "results, run_dir = train_and_evaluate(\n",
    "    model,\n",
    "    loaders,\n",
    "    config,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    model_name=MODEL_NAME,\n",
    "    run_root=PROJECT_ROOT / 'runs',\n",
    ")\n",
    "\n",
    "# Ïã§Ìñâ Í∏∞Î°ù Ï†ÄÏû•\n",
    "RUN_HISTORY.append({\n",
    "    'dataset': DATASET_NAME,\n",
    "    'model': MODEL_NAME,\n",
    "    'results': results,\n",
    "    'run_dir': str(run_dir),\n",
    "    'hyperparameters': {\n",
    "        'dataset_3_ratio': DATASET_3_RATIO,\n",
    "        'dataset_4_ratio': DATASET_4_RATIO,\n",
    "        'train_val_ratio': TRAIN_VAL_RATIO,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'epochs': EPOCHS\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ÌïôÏäµ ÏôÑÎ£å!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Í≤∞Í≥º ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏµúÍ∑º Ïã§Ìñâ Í≤∞Í≥º\n",
    "latest = RUN_HISTORY[-1]\n",
    "\n",
    "print(f\"Dataset: {latest['dataset']}\")\n",
    "print(f\"Model: {latest['model']}\")\n",
    "print(f\"Run directory: {latest['run_dir']}\\n\")\n",
    "\n",
    "# Î©îÌä∏Î¶≠ Ìëú Ï∂úÎ†•\n",
    "metrics_df = pd.DataFrame(latest['results']).T\n",
    "print(\"\\nüìä ÏÑ±Îä• Î©îÌä∏Î¶≠:\")\n",
    "display(metrics_df)\n",
    "\n",
    "# Confusion Matrix ÌëúÏãú\n",
    "cm_path = Path(latest['run_dir']) / 'confusion_matrix.png'\n",
    "if cm_path.exists():\n",
    "    print(\"\\nüìà Confusion Matrix:\")\n",
    "    display(Image(filename=str(cm_path)))\n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è  Confusion matrix not available for this run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ÏÉÅÏÑ∏ Í≤∞Í≥º Î∂ÑÏÑù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = RUN_HISTORY[-1]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ÏÉÅÏÑ∏ Í≤∞Í≥º Î∂ÑÏÑù\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    if split in latest['results']:\n",
    "        metrics = latest['results'][split]\n",
    "        print(f\"\\n{split.upper()} Í≤∞Í≥º:\")\n",
    "        print(f\"  - Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
    "        print(f\"  - Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  - Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"  - F1 Score:  {metrics['f1']:.4f}\")\n",
    "        print(f\"  - AUROC:     {metrics['auroc']:.4f}\")\n",
    "        print(f\"  - Loss:      {metrics['loss']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = RUN_HISTORY[-1]\n",
    "\n",
    "print(\"ÏÇ¨Ïö©Îêú ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in latest['hyperparameters'].items():\n",
    "    if 'ratio' in key and value < 1:\n",
    "        print(f\"{key}: {value} ({value*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ï†ÑÏ≤¥ Ïã§Ìóò Í∏∞Î°ù (ÏÑ†ÌÉùÏÇ¨Ìï≠)\n",
    "\n",
    "Ïó¨Îü¨ Î≤à Ïã§ÌóòÌïú Í≤ΩÏö∞ Î™®Îì† Í≤∞Í≥ºÎ•º ÎπÑÍµêÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(RUN_HISTORY) > 0:\n",
    "    summary_rows = []\n",
    "    \n",
    "    for idx, record in enumerate(RUN_HISTORY):\n",
    "        row = {\n",
    "            'exp_id': idx + 1,\n",
    "            'dataset': record['dataset'],\n",
    "            'model': record['model'],\n",
    "        }\n",
    "        \n",
    "        # ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï∂îÍ∞Ä\n",
    "        if 'hyperparameters' in record:\n",
    "            for key, value in record['hyperparameters'].items():\n",
    "                row[key] = value\n",
    "        \n",
    "        # Validation Î©îÌä∏Î¶≠ Ï∂îÍ∞Ä\n",
    "        val_metrics = record['results'].get('val', {})\n",
    "        for key, value in val_metrics.items():\n",
    "            row[f'val_{key}'] = value\n",
    "        \n",
    "        # Test Î©îÌä∏Î¶≠ Ï∂îÍ∞Ä\n",
    "        test_metrics = record['results'].get('test', {})\n",
    "        for key, value in test_metrics.items():\n",
    "            row[f'test_{key}'] = value\n",
    "        \n",
    "        summary_rows.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    \n",
    "    print(f\"\\nÏ†ÑÏ≤¥ Ïã§Ìóò Í∏∞Î°ù ({len(RUN_HISTORY)}Í∞ú):\")\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Í∞ÄÏû• Ï¢ãÏùÄ Í≤∞Í≥º Ï∞æÍ∏∞\n",
    "    if 'val_f1' in summary_df.columns:\n",
    "        best_idx = summary_df['val_f1'].idxmax()\n",
    "        print(f\"\\nüèÜ Best result (by val_f1):\")\n",
    "        print(summary_df.loc[best_idx])\n",
    "else:\n",
    "    print('ÏïÑÏßÅ Ïã§ÌñâÎêú Ïã§ÌóòÏù¥ ÏóÜÏäµÎãàÎã§. ÏúÑÏùò ÌïôÏäµ ÏÖÄÏùÑ Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
