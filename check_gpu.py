#!/usr/bin/env python3
"""
GPU í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
CUDAê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€, ê·¸ë¦¬ê³  í•™ìŠµì´ GPUì—ì„œ ì‹¤í–‰ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
"""
import torch
import sys

def check_gpu():
    """GPU ìƒíƒœ í™•ì¸"""
    print("=" * 70)
    print("ğŸ–¥ï¸  GPU ìƒíƒœ í™•ì¸")
    print("=" * 70)
    
    # PyTorch ë²„ì „
    print(f"\nğŸ“¦ PyTorch ë²„ì „: {torch.__version__}")
    
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    cuda_available = torch.cuda.is_available()
    print(f"\nğŸ” CUDA ì‚¬ìš© ê°€ëŠ¥: {'âœ… Yes' if cuda_available else 'âŒ No'}")
    
    if cuda_available:
        # GPU ì •ë³´
        print(f"\nğŸ’» GPU ì •ë³´:")
        print(f"   - GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"\n   GPU {i}:")
            print(f"     - ì´ë¦„: {torch.cuda.get_device_name(i)}")
            props = torch.cuda.get_device_properties(i)
            print(f"     - ì´ ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.2f} GB")
            print(f"     - Compute Capability: {props.major}.{props.minor}")
        
        # í˜„ì¬ GPU
        print(f"\nğŸ¯ í˜„ì¬ ì‚¬ìš© GPU: cuda:{torch.cuda.current_device()}")
        
        # ê°„ë‹¨í•œ ì—°ì‚° í…ŒìŠ¤íŠ¸
        print(f"\nğŸ§ª GPU ì—°ì‚° í…ŒìŠ¤íŠ¸:")
        try:
            x = torch.randn(1000, 1000).cuda()
            y = torch.randn(1000, 1000).cuda()
            z = torch.mm(x, y)
            print(f"   âœ… GPU ì—°ì‚° ì„±ê³µ! (í–‰ë ¬ ê³±ì…ˆ í…ŒìŠ¤íŠ¸ í†µê³¼)")
            print(f"   - ì‚¬ìš© ë””ë°”ì´ìŠ¤: {z.device}")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            allocated = torch.cuda.memory_allocated(0) / 1024**2
            cached = torch.cuda.memory_reserved(0) / 1024**2
            print(f"   - í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated:.2f} MB")
            print(f"   - ìºì‹œëœ ë©”ëª¨ë¦¬: {cached:.2f} MB")
            
        except Exception as e:
            print(f"   âŒ GPU ì—°ì‚° ì‹¤íŒ¨: {e}")
            return False
        
        print(f"\nâœ¨ í•™ìŠµ ì‹œ ìë™ìœ¼ë¡œ GPUê°€ ì‚¬ìš©ë©ë‹ˆë‹¤!")
        return True
    
    else:
        print(f"\nâŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"\nğŸ’¡ GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:")
        print(f"   1. NVIDIA GPUê°€ ì‹œìŠ¤í…œì— ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
        print(f"   2. NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜")
        print(f"   3. CUDA Toolkit ì„¤ì¹˜")
        print(f"   4. PyTorch CUDA ë²„ì „ ì¬ì„¤ì¹˜:")
        print(f"      pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        print(f"\nâš ï¸  í˜„ì¬ëŠ” CPUë¡œ í•™ìŠµë©ë‹ˆë‹¤ (ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        return False
    
    print("=" * 70)

def check_cudnn():
    """cuDNN ìƒíƒœ í™•ì¸"""
    if torch.cuda.is_available():
        print(f"\nğŸ”§ cuDNN:")
        print(f"   - cuDNN ì‚¬ìš© ê°€ëŠ¥: {'âœ… Yes' if torch.backends.cudnn.is_available() else 'âŒ No'}")
        if torch.backends.cudnn.is_available():
            print(f"   - cuDNN ë²„ì „: {torch.backends.cudnn.version()}")
            print(f"   - cuDNN Enabled: {torch.backends.cudnn.enabled}")

if __name__ == "__main__":
    print("\n")
    success = check_gpu()
    check_cudnn()
    print("\n")
    
    # ì¢…ë£Œ ì½”ë“œ
    sys.exit(0 if success else 1)

